{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download csv and shape files from https://hub.worldpop.org/geodata/summary?id=50577\n",
    "\n",
    "Metadata for MigrEst_international_v7.csv\n",
    "14/11/2019 Dorothea Woods, University of Southampton\n",
    "\n",
    "The file contains estimated international migration movements between countries on a subnational level which were produced\n",
    "for the project \"Mapping gender-disaggregated migration movements at subnational scales in and between low- and middle-income countries\".\n",
    "\n",
    "The file contains 18,679,720 records and 8 columns, these are described below:\n",
    "\tISOI - 3 letter ISO code of origin country\n",
    "\tNODEI - ID of origin admin unit\n",
    "\tISOJ - 3 letter ISO code of destination country\n",
    "\tNODEJ - ID of destination admin unit\n",
    "\tsex - sex of migrants, 'M' or 'F'\n",
    "\tpred_seed1 - estimated number of migrants between origin and destination country and admin unit calculated using IPF seed = 1\n",
    "\tpred_dist - estimated number of migrants between origin and destination country and admin unit calculated using IPF seed = distance \n",
    "\tpred_grav - estimated number of migrants between origin and destination country and admin unit calculated using IPF seed = (TotPopI * TotPopJ) / distance \n",
    "\n",
    "For visualization purposes the migration estimates can be joined to Flowlines_International.shp using the information in the first 4 columns ISOI, NODEI, ISOJ, NODEJ.\n",
    "\n",
    "For details regarding the production of the estimates please refer to the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "csv_file_path = r\"C:\\Users\\paude\\Downloads\\SexDisaggregated_Migration\\SexDisaggregated_Migration\\MigrationEstimates\\MigrEst_international_v7.csv\"  # Update with your CSV file path\n",
    "shapefile_path = r\"C:\\Users\\paude\\Downloads\\SexDisaggregated_Migration\\SexDisaggregated_Migration\\SpatialData\\Flowlines_International.shp\"  # Update with your shapefile path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in CSV key columns:\n",
      "ISOI     0\n",
      "NODEI    0\n",
      "ISOJ     0\n",
      "NODEJ    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the key columns\n",
    "print(\"Missing values in CSV key columns:\")\n",
    "print(df[['ISOI', 'NODEI', 'ISOJ', 'NODEJ']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while processing shapefile: Invalid offset for entity 17339\n"
     ]
    }
   ],
   "source": [
    "# Attempt to read the shapefile using geopandas\n",
    "try:\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    print(\"\\nShapefile loaded successfully:\")\n",
    "    print(gdf.head())\n",
    "\n",
    "    # Check for missing values in the shapefile key columns\n",
    "    print(\"\\nMissing values in shapefile key columns:\")\n",
    "    print(gdf[['ISO', 'NODE']].isnull().sum())\n",
    "\n",
    "    # Merge the CSV data with the shapefile based on the origin node\n",
    "    merged_gdf_origin = gdf.merge(df, left_on=['ISO', 'NODE'], right_on=['ISOI', 'NODEI'], how='inner')\n",
    "\n",
    "    # Check if the merge was successful\n",
    "    print(\"\\nMerged data (origin nodes) - first few rows:\")\n",
    "    print(merged_gdf_origin.head())\n",
    "\n",
    "    # Merge the CSV data with the shapefile based on the destination node\n",
    "    merged_gdf_destination = gdf.merge(df, left_on=['ISO', 'NODE'], right_on=['ISOJ', 'NODEJ'], how='inner')\n",
    "\n",
    "    # Check if the merge was successful\n",
    "    print(\"\\nMerged data (destination nodes) - first few rows:\")\n",
    "    print(merged_gdf_destination.head())\n",
    "\n",
    "    # Save the merged GeoDataFrames to new shapefiles (optional)\n",
    "    merged_gdf_origin.to_file('merged_origin.shp')\n",
    "    merged_gdf_destination.to_file('merged_destination.shp')\n",
    "\n",
    "    print(\"Merged shapefiles saved as 'merged_origin.shp' and 'merged_destination.shp'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while processing shapefile: {e}\")\n",
    "\n",
    "# Continue with the rest of the script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inspect unique values to ensure they match\n",
    "print(\"\\nUnique values in CSV 'ISOI' column:\")\n",
    "print(df['ISOI'].unique())\n",
    "\n",
    "print(\"\\nUnique values in shapefile 'ISO' column:\")\n",
    "print(gdf['ISO'].unique())\n",
    "\n",
    "# If column names in shapefile are different, adjust accordingly\n",
    "# Display the structure of the shapefile to understand its contents\n",
    "print(\"\\nShapefile structure:\")\n",
    "print(gdf.head())\n",
    "\n",
    "# Merge the CSV data with the shapefile based on the origin node\n",
    "merged_gdf_origin = gdf.merge(df, left_on=['ISO', 'NODE'], right_on=['ISOI', 'NODEI'], how='inner')\n",
    "\n",
    "# Check if the merge was successful\n",
    "print(\"\\nMerged data (origin nodes) - first few rows:\")\n",
    "print(merged_gdf_origin.head())\n",
    "\n",
    "print(\"\\nMerged data (origin nodes) - shape:\")\n",
    "print(merged_gdf_origin.shape)\n",
    "\n",
    "# Merge the CSV data with the shapefile based on the destination node\n",
    "merged_gdf_destination = gdf.merge(df, left_on=['ISO', 'NODE'], right_on=['ISOJ', 'NODEJ'], how='inner')\n",
    "\n",
    "# Check if the merge was successful\n",
    "print(\"\\nMerged data (destination nodes) - first few rows:\")\n",
    "print(merged_gdf_destination.head())\n",
    "\n",
    "print(\"\\nMerged data (destination nodes) - shape:\")\n",
    "print(merged_gdf_destination.shape)\n",
    "\n",
    "# Save the merged GeoDataFrames to new shapefiles (optional)\n",
    "merged_gdf_origin.to_file('merged_origin.shp')\n",
    "merged_gdf_destination.to_file('merged_destination.shp')\n",
    "\n",
    "print(\"Merged shapefiles saved as 'merged_origin.shp' and 'merged_destination.shp'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
